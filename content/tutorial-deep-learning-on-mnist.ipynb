{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxsbqCr6gvMe"
   },
   "source": [
    "# Tutorial: Deep learning on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj0asxECcn_f"
   },
   "source": [
    "This tutorial demonstrates how to build and train a simple 3-layer [feed-forward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network) with [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) learning from scratch.\n",
    "\n",
    "The deep learning model that you will construct with NumPy will learn from the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, which contains 60,000 training and 10,000 test images and corresponding labels of handwritten digits from 0 to 9. In this tutorial you will learn how to parse and load the dataset without the aid of external deep learning libraries. Each training and test image is of size 784 (or 28x28 pixels) — this will be your input for the neural network. The images have corresponding labels of digits from 0 to 9 (10 in total).\n",
    "\n",
    "Based on the image inputs and their labels, your neural network will be trained to learn their features using forward propagation and backpropagation (reverse-mode differentiation). The final output of the network is a vector of 10 scores — one for each handwritten digit image. You will also evaluate how good your model is at classifying the images on the test set. This type of machine or deep learning is often referred to as [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).\n",
    "\n",
    "This tutorial was adapted from the work by [Andrew Trask](https://github.com/iamtrask/) (with the author's permission) and inspired by the teachings of [Joel Grus](https://github.com/joelgrus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bETeUhIP5PKA"
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b0vuABT5Q_i"
   },
   "source": [
    "The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. In addition, you should be familiar with main concepts of [deep learning](https://en.wikipedia.org/wiki/Deep_learning). \n",
    "\n",
    "To refresh the memory, you can take the [Python](https://docs.python.org/dev/tutorial/index.html) and [Linear algebra on n-dimensional arrays](https://numpy.org/doc/stable/user/tutorial-svd.html) tutorials. \n",
    "\n",
    "You are advised to read the [Deep learning](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field.\n",
    "\n",
    "In addition to NumPy, you will be utilizing the following Python standard modules for data loading and processing: \n",
    "- [`urllib`](https://docs.python.org/3/library/urllib.html) for URL handling.\n",
    "- [`request`](https://docs.python.org/3/library/urllib.request.html) for URL opening.\n",
    "- [`gzip`](https://docs.python.org/3/library/gzip.html) for gzip file decompression.\n",
    "- [`pickle`](https://docs.python.org/3/library/pickle.html) to work with the pickle file format.\n",
    "\n",
    "    as well as:\n",
    "- [matplotlib](https://matplotlib.org/) for data visualization.\n",
    "\n",
    "This tutorial can be run locally in an isolated environment, such as [Virtualenv](https://virtualenv.pypa.io/en/stable/). You can use [Jupyter Notebook or JupyterLab](https://jupyter.org/install) to run each notebook cell. Don't forget to [set up NumPy](https://numpy.org/doc/stable/user/absolute_beginners.html#installing-numpy) and [matplotlib](https://matplotlib.org/users/installing.html#installing-an-official-release)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwh28_wA5Ehm"
   },
   "source": [
    "### Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waBVnvMF5HP1"
   },
   "source": [
    "1. Load the MNIST dataset\n",
    "\n",
    "2. Preprocess the dataset\n",
    "\n",
    "3. Build and train a small neural network from scratch\n",
    "\n",
    "4. Next steps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvczBaGDilbb"
   },
   "source": [
    "## 1. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m3eY0XpoHNB"
   },
   "source": [
    "In this section, you will download the MNIST dataset and transform it into 4 files of NumPy arrays using built-in Python modules. Then, you will split the arrays into training and test sets.\n",
    "\n",
    "(Credit to [hsjeong5](https://github.com/hsjeong5/MNIST-for-Numpy) for demonstrating how to carry out these steps without the use of external libraries.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx_wxOpdOIU8"
   },
   "source": [
    "1. Download and prepare the data:\n",
    "\n",
    "First, we download and store the zipped files, originally stored in [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). For this, we'll define a variable to store the training/test image/label names of the MNIST dataset in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j0jUSHTmfUdC"
   },
   "outputs": [],
   "source": [
    "filename = [[\"training_images\", \"train-images-idx3-ubyte.gz\"],   # 60,000 training images.\n",
    "            [\"test_images\", \"t10k-images-idx3-ubyte.gz\"],        # 10,000 test images.\n",
    "            [\"training_labels\", \"train-labels-idx1-ubyte.gz\"],   # 60,000 training labels.\n",
    "            [\"test_labels\", \"t10k-labels-idx1-ubyte.gz\"]]        # 10,000 test labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll download each of the four files in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: train-images-idx3-ubyte.gz\n",
      "Downloading file: t10k-images-idx3-ubyte.gz\n",
      "Downloading file: train-labels-idx1-ubyte.gz\n",
      "Downloading file: t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "for name in filename:\n",
    "    print(\"Downloading file: \" + name[1])\n",
    "    request.urlretrieve(base_url + name[1], name[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx_wxOpdOIU8"
   },
   "source": [
    "Next, we extract and save the data to disk, using NumPy for the first time to read the contents of each file as [ndarrays](https://numpy.org/doc/stable/reference/arrays.ndarray.html) and saving them into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "mnist_dataset = {}\n",
    "for name in filename[:2]:\n",
    "    with gzip.open(name[1], 'rb') as mnist_file:\n",
    "        mnist_dataset[name[0]] = np.frombuffer(mnist_file.read(), np.uint8, offset=16).reshape(-1, 28*28)\n",
    "for name in filename[-2:]:\n",
    "    with gzip.open(name[1], 'rb') as mnist_file:\n",
    "        mnist_dataset[name[0]] = np.frombuffer(mnist_file.read(), np.uint8, offset=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc2aZW_VmtHo"
   },
   "source": [
    "2. Split the images and labels into training and test sets.\n",
    "\n",
    "For this, we'll use the standard notation and call the training and test set of images as `x_train` and `x_test`, and the set of labels as `y_train` and `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_lVIsI25jDzq"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = (mnist_dataset[\"training_images\"],\n",
    "                                    mnist_dataset[\"training_labels\"],\n",
    "                                    mnist_dataset[\"test_images\"],\n",
    "                                    mnist_dataset[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we have the right data, we can print the shape of the sets we just built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EH65tTeqak-L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training images: (60000, 784) and training labels: (60000,)\n",
      "The shape of test images: (10000, 784) and test labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of training images: {} and training labels: {}'.format(x_train.shape, y_train.shape))\n",
    "print('The shape of test images: {} and test labels: {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGrH_Vgq1_uP"
   },
   "source": [
    "3. (Optional) You can inspect some samples of the image data with matplotlib and image labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nidzt93yQua2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Take the 60,000th image (index 59999) from the training set, reshape to (28x28) if necessary.\n",
    "mnist_image = x_train[59999, :].reshape(28, 28)\n",
    "# Set the color mapping to grayscale.\n",
    "plt.imshow(mnist_image, cmap='gray')\n",
    "# Display the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7PsbI0fhH5yu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display 5 random images from the training set.\n",
    "np.random.seed(0)\n",
    "indices = list(np.random.randint(x_train.shape[0], size=9))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(x_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVVMdJ5eRV10"
   },
   "source": [
    "> **Note:** You can also visualize a sample image as an array by printing `x_train[59999]`. Here, `59999` is your 60,000th training image sample (`0` would be your first). Your output will be quite long and should contain an array of 8-bit integers:\n",
    ">\n",
    "> \n",
    "> ```\n",
    "> ...\n",
    ">          0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,   0,\n",
    ">          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    ">          0,  62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,\n",
    "> ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7xL14kXGeLX0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the label of the 60,000th image (index 59999) from the training set.\n",
    "y_train[59999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gbciL8U2JpJ"
   },
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "598v8kPF8s6e"
   },
   "source": [
    "You have loaded the MNIST dataset and saved it to disk as 4 files:\n",
    "\n",
    "- `training_images`: flattened training images of size 60,000x784.\n",
    "- `training_labels`: labels corresponding to the training images of size 1x60,000.\n",
    "- `test_images`: flattened test images of size 60,000x784.\n",
    "- `test_labels`: labels corresponding to the training images of size 1x60,000.\n",
    "\n",
    "Neural networks can work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type. Since the MNIST data arrays are of `dtype` `uint8`, your next challenge is to convert them to floating points, such as `float64`.\n",
    "\n",
    "> **Note:** Data preparation in deep learning is a very challenging process that can take a lot of time, especially if the data is in a more raw form. When preprocessing the data, you should consider the following processes:\n",
    "> \n",
    "> - [_Vectorization_](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29): The data should be vectorized and, luckily, the MNIST dataset already is.\n",
    "> \n",
    "> - [_Conversion to floating points_](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Floating-point_numbers)_: Neural networks can work with data that is of floating-point type.\n",
    "> \n",
    ">    In practice, you can use different types of floating-point precision depending on your goals and you can find more information about that in the [Nvidia](https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/) and [Google Cloud](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus) blog posts.\n",
    "\n",
    "Since your dataset is already represented by arrays containing 8-bit integers, let's convert their `dtype` to floating points, such as `float64` — 64-bit (or [double-precision](https://en.wikipedia.org/wiki/Double-precision_floating-point_format)) — by doing the following:\n",
    "\n",
    "- _Normalizing_ the image data. \n",
    "\n",
    "- _[One-hot/categorical encoding](https://en.wikipedia.org/wiki/One-hot)_ of the image labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_36s7MmeLSWi"
   },
   "source": [
    "### Convert the image data to floating points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GfBDHi2ciWS"
   },
   "source": [
    "The images data contain 8-bit integers encoded in the [0, 255] interval with color values between 0 and 255. \n",
    "\n",
    "You will normalize them into floating-point arrays in the [0, 1] interval by dividing them by 255.\n",
    "\n",
    "> **Note:** Since the numbers inside matrices will be smaller, it should also make it faster for the neural network to learn from that data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIOaSkL1_Ng9"
   },
   "source": [
    "1. (Optional) Check that the vectorized image data has type `uint8`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MCdAEvVrdeBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training images: uint8\n",
      "The data type of test images: uint8\n"
     ]
    }
   ],
   "source": [
    "print('The data type of training images: {}'.format(x_train.dtype))\n",
    "print('The data type of test images: {}'.format(x_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-Wym6KKvc5"
   },
   "source": [
    "2. Normalize by dividing the arrays by 255 and assign the train and test image data variables — `x_train` and `x_test` — to `training_images` and `train_labels`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RqEhJqQPRwVs"
   },
   "outputs": [],
   "source": [
    "# Transform the arrays in the [0, 1] interval by dividing them by 255.\n",
    "sample = 1000\n",
    "training_images = x_train[0:sample].reshape(sample, 28*28) / 255\n",
    "test_images = x_test.reshape(len(x_test), 28*28) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z18GqfDImIhq"
   },
   "source": [
    "> **Note:** To make the neural network model train faster in this example, `training_images` contains only 1,000 samples out of 60,000. To learn from the entire sample size, change the `sample` variable to `60000`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VqAgiLrLEDn"
   },
   "source": [
    "3. Confirm that the image data has changed to the floating-point format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "j0bDv7snLYd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training images: float64\n",
      "The data type of test images: float64\n"
     ]
    }
   ],
   "source": [
    "print('The data type of training images: {}'.format(training_images.dtype))\n",
    "print('The data type of test images: {}'.format(test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGN1HOrkyLxo"
   },
   "source": [
    "> **Note:** You can also check that normalization was successful by printing `training_images[0]` in a notebook cell. Your long output should contain an array of floating point numbers:\n",
    "> \n",
    "> ```\n",
    "> ...\n",
    ">        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
    "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
    "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
    "> ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I14bhyapTUwH"
   },
   "source": [
    "### Convert the labels to floating points through categorical/one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibNYe02-jwgo"
   },
   "source": [
    "You will use one-hot encoding to embed each digit label as an all-zero vector with `np.zeros()` and place `1` for a label index. As a result, your label data will be arrays with `1.0` (or `1.`) in the position of each image label.\n",
    "\n",
    "Since there are 10 labels (from 0 to 9) in total, your arrays will look similar to this: \n",
    "\n",
    "```\n",
    "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_2hN6QD_fe0"
   },
   "source": [
    "1. (Optional) Confirm that the image label data are integers with `dtype` `uint8`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xsU0lym3_jmh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training labels: uint8\n",
      "The data type of test labels: uint8\n"
     ]
    }
   ],
   "source": [
    "print('The data type of training labels: {}'.format(y_train.dtype))\n",
    "print('The data type of test labels: {}'.format(y_test.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_3tOAryO-V5"
   },
   "source": [
    "2. Define a function that performs one-hot encoding on arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nPI2lLwbbgEx"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, dimension=10):\n",
    "    \"\"\"One-hot encode arrays.\"\"\"\n",
    "    # Define a one-hot variable for an all-zero vector \n",
    "    # with 10 dimensions (number labels from 0 to 9).\n",
    "    one_hot_labels = np.zeros((len(labels), dimension))\n",
    "    # Embed each label as an all-zero vector and...\n",
    "    for i, label in enumerate(labels):\n",
    "        # ... place `1` for a label index.\n",
    "        one_hot_labels[i][label] = 1 \n",
    "    # Return one-hot encoded labels.\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlb7n47hPJ-Y"
   },
   "source": [
    "3. Use the newly created function to one-hot encode the image labels and assign the train and test label data variables — `y_train` and `y_test` — to `training_labels` and `test_labels`, respectively:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LUfru5XHjj7_"
   },
   "outputs": [],
   "source": [
    "training_labels = one_hot_encoding(y_train)\n",
    "test_labels = one_hot_encoding(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qyd-hMilPUo1"
   },
   "source": [
    "4. Check that the data type has changed to floating point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6yIdQ9gFfMKu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of training labels: float64\n",
      "The data type of test labels: float64\n"
     ]
    }
   ],
   "source": [
    "print('The data type of training labels: {}'.format(training_labels.dtype))\n",
    "print('The data type of test labels: {}'.format(test_labels.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZPmdsSFPZlp"
   },
   "source": [
    "5. (Optional) Visualize a few samples of the one-hot encoded labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5-88KNPWOGcp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(training_labels[0])\n",
    "print(training_labels[1])\n",
    "print(training_labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqu-A-mXj-RK"
   },
   "source": [
    "and compare them to the original labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TdQ-6wClj3aC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-X_JJtjXGH2"
   },
   "source": [
    "You have finished preparing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w_7E2AQhAaJ"
   },
   "source": [
    "## 3. Build and train a small neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMfO1_W8XpQX"
   },
   "source": [
    "In this section you will familiarize with some high-level concepts of the basic building blocks of a deep learning model. You can refer to the original [Deep learning](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) research publication for more information.\n",
    "\n",
    "Afterwards, you will construct the building blocks of a simple deep learning model in Python and NumPy and train it to learn to identify handwritten digits from the MNIST dataset with a certain level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUIh7PIDXd_3"
   },
   "source": [
    "### Neural network building blocks with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmSBCqi9XuJd"
   },
   "source": [
    "- _Layers_: These building blocks work as data filters — they process data and learn representations from inputs to better predict the target outputs.\n",
    "\n",
    "    You will use 3 layers in your model to pass the inputs forward (_forward propagation_) and propagate the gradients/error derivatives of a loss function backward (_backpropagation_). These are input, hidden and output layers.\n",
    "    \n",
    "    In the hidden (middle) and output (last) layers, the neural network model will compute the weighted sum of inputs. To compute this process, you will use NumPy's matrix multiplication module (the \"dot multiply\" or `np.dot(layer, weights)`).\n",
    "\n",
    "    > **Note:** For simplicity, the bias term is omitted in this example (there is no `np.dot(layer, weights) + bias`).\n",
    "\n",
    "- _Weights_: These are important adjustable parameters that the neural network fine-tunes by forward and backward propagating the data. \n",
    "    \n",
    "    The optimal weights should produce the highest prediction accuracy and the lowest error on the training and test sets. Before the model training starts, the weights are randomly initialized with NumPy's `np.random.random()` module.\n",
    "\n",
    "- _Activation function_: Deep learning models are capable of determining non-linear relationships between inputs and inputs and these [non-linear functions](https://en.wikipedia.org/wiki/Activation_function) are usually applied to the output of each layer.\n",
    "\n",
    "    You will use a [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) to the hidden layer's output (for example, `relu(np.dot(layer, weights))`.\n",
    "\n",
    "- _Regularization_: This [technique](https://en.wikipedia.org/wiki/Regularization_(mathematics) helps prevent the neural network model from [overfitting](https://en.wikipedia.org/wiki/Overfitting). \n",
    "\n",
    "    In this example, you will use a method called dropout ([dilution](https://en.wikipedia.org/wiki/Dilution_(neural_networks))) that randomly sets a number of features in a layer to 0s. You will define it with NumPy's `np.random.randint()` module and apply it to the hidden layer of the network.\n",
    "\n",
    "- _Loss function_: The computation determines the quality of predictions by comparing the image labels (the truth) with the predicted values in the final layer's output.\n",
    "\n",
    "    For simplicity, you will use a basic total-squared error using NumPy's `np.sum()` function (for example, `np.sum((final_layer_output - image_labels) ** 2)`).\n",
    "\n",
    "- _Accuracy_: This metric measures the accuracy of the network's ability to predict on the data it hasn't seen.\n",
    "\n",
    "- _Forward propagation, backpropagation, training loop_: \n",
    "\n",
    "    In the beginning of model training, your network randomly initializes the weights and feeds the input data forward from the first to last layers. This process is the forward pass or forward propagation. \n",
    "    \n",
    "    Then, the network propagates the \"signal\" from the loss function back through the hidden layer and adjusts the weights values with the help of the alpha parameter (more on that later). \n",
    "    \n",
    "> **Note:** In more technical terms, you: \n",
    ">    \n",
    ">    1. Measure the error by comparing the real label of an image (the truth) with the prediction of the model.\n",
    ">    2. Differentiate the loss function.\n",
    ">    3. Ingest the [gradients](https://en.wikipedia.org/wiki/Gradient) with the respect to the output, and backpropagate them with the respect to the inputs through the layer(s). \n",
    ">    \n",
    ">    Since the network contains tensor operations and weight matrices, backpropagation uses the [chain rule](https://en.wikipedia.org/wiki/Chain_rule).\n",
    ">\n",
    ">    With each iteration (epoch) of the neural network training, this forward and backward propagation cycle adjusts the weights, which is reflected in the accuracy and error metrics. As you train the model, your goal is to minimize the error and maximize the accuracy on the training data, where the model learns from, as well as the test data, where you evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmd3KfiAXjKJ"
   },
   "source": [
    "### Model architecture summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErogxYX1kl3j"
   },
   "source": [
    "Here is a simplified illustration of the neural network model architecture:\n",
    "\n",
    "```\n",
    "    Input data\n",
    "\n",
    "        -> Input layer\n",
    "        -> Hidden layer\n",
    "        -> ReLU\n",
    "        -> Dropout\n",
    "        -> Output layer\n",
    "\n",
    "    -> Outputs with predictions\n",
    "\n",
    "```\n",
    "\n",
    "- _The input layer_: \n",
    "\n",
    "    The first layer of the network represents the previously preprocessed data that is loaded from `training_images` into `layer_0`.\n",
    "\n",
    "- _The hidden (middle) layer_: \n",
    "\n",
    "    `layer_1` takes the output from the previous layer and performs matrix-multiplication of the inputs by weights (`weights_1`) with NumPy's `np.dot()`).\n",
    "\n",
    "    Then, this output is passed through the ReLU activation function for non-linearity and then dropout is applied to help with overfitting.\n",
    "\n",
    "- _The output (last) layer_: \n",
    "\n",
    "    `layer_2` ingests the output from `layer_1` and repeats the same \"dot multiply\" process with `weights_2`.\n",
    "\n",
    "    The final output returns 10 scores for each of the 0-9 digit labels. The network model ends with a size 10 layer — a 10-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q62G3Dm3X_Iu"
   },
   "source": [
    "### Compose the model and begin training and testing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClYLfOu-YbQH"
   },
   "source": [
    "Having covered the main deep learning concepts and the neural network architecture, let's write the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMNspYCWo8C4"
   },
   "source": [
    "1. For reproducibility, initialize a random seed with `np.random.seed()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Vwuj03SzknJz"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcWHlf9Io_Vp"
   },
   "source": [
    "2. For the hidden layer, define the ReLU activation function for forward propagation and ReLU's derivative that will be used during backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FH8OCrWnikQr"
   },
   "outputs": [],
   "source": [
    "# Define ReLU that returns the input if it's positive and 0 otherwise.\n",
    "def relu (x):\n",
    "    return (x>=0) * x\n",
    "\n",
    "# Set up a derivative of the ReLU function that returns 1 for a positive input \n",
    "# and 0 otherwise.\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL0kHvJtpkZ-"
   },
   "source": [
    "3. Set certain default values of [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)), such as:\n",
    "\n",
    "- _Alpha_: `alpha` — helps prevent the weights from overcorrecting during the updates.\n",
    "- _Epochs (iterations)_: `epochs` — the number of complete passes — forward and backward propagations — of the data through the network. This parameter can positively or negatively affect the results. The higher the iterations, the longer the learning process may take.\n",
    "- _Size of the hidden (middle) layer in a network_: `hidden_size` — different sizes of the hidden layer can affect the results during training and testing.\n",
    "- _Size of the input:_ `pixels_per_image` — you have established that the image input is 784 (28x28) (in pixels).\n",
    "- _Number of labels_: `num_labels` — indicates the output number for the output layer where the predictions occur for 10 (0 to 9) handwritten digit labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w5herUOalnW1"
   },
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "epochs = 100\n",
    "hidden_size = 100\n",
    "pixels_per_image = 784\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZGa4JgVqRbs"
   },
   "source": [
    "5. Initialize the weight vectors that will used for each the middle and output layers with NumPy's `random.random()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4m-YRwtcksg8"
   },
   "outputs": [],
   "source": [
    "weights_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DVkEgH3qyMR"
   },
   "source": [
    "6. Set up the neural network's learning experiment with a training loop and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "oa_ToZ7Ok1Js"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:10 Training set error:0.650 Training set accuracy:0.62 Test set error:0.578 Test set accuracy:0.6835\n",
      "\n",
      "Epoch:20 Training set error:0.460 Training set accuracy:0.779 Test set error:0.439 Test set accuracy:0.7984\n",
      "\n",
      "Epoch:30 Training set error:0.436 Training set accuracy:0.801 Test set error:0.435 Test set accuracy:0.8042\n",
      "\n",
      "Epoch:40 Training set error:0.424 Training set accuracy:0.813 Test set error:0.428 Test set accuracy:0.8043\n",
      "\n",
      "Epoch:50 Training set error:0.410 Training set accuracy:0.85 Test set error:0.420 Test set accuracy:0.8102\n",
      "\n",
      "Epoch:60 Training set error:0.398 Training set accuracy:0.838 Test set error:0.412 Test set accuracy:0.8121\n",
      "\n",
      "Epoch:70 Training set error:0.390 Training set accuracy:0.848 Test set error:0.414 Test set accuracy:0.823\n",
      "\n",
      "Epoch:80 Training set error:0.380 Training set accuracy:0.844 Test set error:0.411 Test set accuracy:0.8115\n",
      "\n",
      "Epoch:90 Training set error:0.378 Training set accuracy:0.863 Test set error:0.410 Test set accuracy:0.8109\n",
      "\n",
      "Epoch:100 Training set error:0.365 Training set accuracy:0.851 Test set error:0.411 Test set accuracy:0.7982\n"
     ]
    }
   ],
   "source": [
    "# This is a training loop.\n",
    "# Run the learning experiment for a defined number of epochs (iterations).\n",
    "for j in range(epochs):\n",
    "    # Set the initial loss/error and the number of accurate predictions to zero.\n",
    "    training_loss = 0.0\n",
    "    training_accurate_predictions = 0\n",
    "    \n",
    "    # For all images in the training set, perform a forward pass\n",
    "    # and backpropagation and adjust the weights accordingly.\n",
    "    for i in range(len(training_images)):\n",
    "        # Forward propagation/forward pass:\n",
    "        # 1. The input layer:\n",
    "        #    Initialize the training image data as the first layer.\n",
    "        layer_0 = training_images[i:i+1]\n",
    "        # 2. The hidden layer:\n",
    "        #    Take in the training image data into the middle layer by \n",
    "        #    matrix-multiplying it by randomly initialized weights. \n",
    "        layer_1 = np.dot(layer_0, weights_1)\n",
    "        # 3. Pass the hidden layer's output through the ReLU activation function.\n",
    "        layer_1 = relu(layer_1)\n",
    "        # 4. Define the dropout function for regularization.\n",
    "        dropout_mask = np.random.randint(0, high=2, size=layer_1.shape)\n",
    "        # 5. Apply dropout to the hidden layer's output.\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        # 6. The output layer:\n",
    "        #    Ingest the output of the middle layer into the the final layer\n",
    "        #    by matrix-multiplying it by randomly initialized weights.\n",
    "        #    Produce a 10-dimension vector with 10 scores.\n",
    "        layer_2 = np.dot(layer_1, weights_2)\n",
    "\n",
    "        # Backpropagation/backward pass:\n",
    "        # 1. Measure the training error (loss function) between the actual\n",
    "        #    image labels (the truth) and the prediction by the model.\n",
    "        training_loss += np.sum((training_labels[i:i+1] - layer_2) ** 2)\n",
    "        # 2. Increment the correct accuracy predictions.\n",
    "        training_accurate_predictions += int(np.argmax(layer_2) == np.argmax(training_labels[i:i+1]))\n",
    "        # 3. Differentiate the loss function/error.\n",
    "        layer_2_delta = (training_labels[i:i+1] - layer_2)\n",
    "        # 3. Propagate the gradients of the loss function back through the hidden layer.\n",
    "        layer_1_delta = layer_2_delta.dot(weights_2.T) * relu2deriv(layer_1)\n",
    "        # 5. Apply the dropout to the gradients.\n",
    "        layer_1_delta *= dropout_mask\n",
    "        # 6. Update the weights for the middle and input layers\n",
    "        #    by multiplying them by alpha and the gradients.\n",
    "        weights_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    # For every 10 epochs:\n",
    "    if(j % 10 == 0):\n",
    "        # 1. Set the initial error and the number of accurate predictions to zero.\n",
    "        test_loss = 0.0\n",
    "        test_accurate_predictions = 0\n",
    "        \n",
    "        # 2. Start testing the model by evaluating on the test image dataset.\n",
    "        for i in range(len(test_images)):\n",
    "            # 1. Pass the test images through the input layer.\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            # 2. Compute the weighted sum of the test image inputs in and\n",
    "            #    pass the hidden layer's output through ReLU.\n",
    "            layer_1 = relu(np.dot(layer_0,weights_1))\n",
    "            # 3. Compute the weighted sum of the hidden layer's inputs.\n",
    "            #    Produce a 10-dimension vector with 10 scores.\n",
    "            layer_2 = np.dot(layer_1, weights_2)\n",
    "\n",
    "            # 4. Measure the error between the actual label (truth) and prediction values.\n",
    "            test_loss += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            # 5. Increment the accurate prediction count.\n",
    "            test_accurate_predictions += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "        # 3. Display the error and accuracy metrics in the output.\n",
    "        print(\"\\n\" + \\\n",
    "              \"Epoch:\" + str(j+10) + \\\n",
    "              \" Training set error:\" + str(training_loss/ float(len(training_images)))[0:5] +\\\n",
    "              \" Training set accuracy:\" + str(training_accurate_predictions/ float(len(training_images))) +\\\n",
    "              \" Test set error:\" + str(test_loss/ float(len(test_images)))[0:5] +\\\n",
    "              \" Test set accuracy:\" + str(test_accurate_predictions/ float(len(test_images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGku-7k5b1b1"
   },
   "source": [
    "The training process may take many minutes, depending on a number of factors, such as the processing power of the machine you are running the experiment on and the number of epochs. To reduce the waiting time, you can change the epoch (iteration) variable from 100 to a lower number, reset the runtime, and run the notebook cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGheVBrPjgT1"
   },
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFvZJYcBoC3-"
   },
   "source": [
    "You have learned how to build and train a simple 3-layer feed-forward neural network from scratch using just NumPy to classify handwritten MNIST digits.\n",
    "\n",
    "The accuracy rates that your model reaches during training may be somewhat plausible but you may also find the error rate to be high. \n",
    "\n",
    "To enhance and optimize your small neural network model, you can consider one of a mixture of the following:\n",
    "- Increase the training sample size from 1,000 to a higher number (up to 60,000).\n",
    "- Alter the architecture by introducing more hidden layers to make the network [deeper](https://en.wikipedia.org/wiki/Deep_learning).\n",
    "- Introduce convolutional layers and replace the feedforward network with a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) architecture.\n",
    "- Use a higher epoch size to train longer and add more regularization techniques, such as [early stopping](https://en.wikipedia.org/wiki/Early_stopping), to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "- Introduce a [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) for an unbiased valuation of the model fit.\n",
    "- Change how you measure the loss by using, for example, categorical [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "- Combine the cross-entropy loss function with a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function in the last layer.\n",
    "- Apply [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) to make for faster and more stable training.\n",
    "- Tune other parameters, such as alpha and hidden layer size.\n",
    "\n",
    "Finally, you can go beyond NumPy with specialized frameworks and APIs — such as [TensorFlow](https://www.tensorflow.org/guide/tf_numpy?hl=el), [PyTorch](https://pytorch.org/docs/stable/generated/torch.from_numpy.html), Swift for TensorFlow (with [Python interoperability](https://www.tensorflow.org/swift/tutorials/python_interoperability)), and [JAX](https://github.com/google/jax) — that support NumPy, have built-in [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), and are designed for high-performance numerical computing and machine learning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial-deep-learning-on-mnist.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
