{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i7nPwJw_gKw"
      },
      "source": [
        "# Tutorial: deep reinforcement learning with Pong from pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI0cmQODX4jL"
      },
      "source": [
        "This tutorial demonstrates how to implement a deep reinforcement learning (RL) agent from scratch using a policy gradient method that learns to play the [Pong](https://gym.openai.com/envs/Pong-v0/) video game using screen pixels as inputs with NumPy. Your Pong agent will obtain experience on the go using an [artificial neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) as its [policy](https://en.wikipedia.org/wiki/Reinforcement_learning).\n",
        "\n",
        "Pong is a 2D game from 1972 where two players use \"rackets\" to play a form of table tennis with a ball. Each player moves the racket up and down the screen and tries to hit a ball in their opponent's direction by touching it. The goal is to hit the ball, such that it goes past the opponent's racket (they miss their shot). According to the rules, if a player reaches 21 points, they win. In Pong, the RL agent that learns to play against an opponent is displayed on the right.\n",
        "\n",
        "This example is based on the [code](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5) developed by [Andrej Karpathy](https://karpathy.ai) for the [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/home) in 2017 at UC Berkeley. His [blog post](http://karpathy.github.io/2016/05/31/rl/) from 2016 also provides more background on the mechanics and theory used in Pong RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G08gAJigcvs2"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb-pZNHWczRy"
      },
      "source": [
        "- **OpenAI Gym**\n",
        "\n",
        "    To help with the game environment, you will use [Gym](https://gym.openai.com) — an open-source Python interface [developed by OpenAI](https://arxiv.org/abs/1606.01540) that helps perform RL tasks while supporting many simulation environments.\n",
        "\n",
        "    You can install OpenAI Gym with PiPy (`pip install gym`) and if you run into any issues, you can refer to the [installation section](https://github.com/openai/gym#installation) in the main repository on GitHub. You can also check the Gym [documentation](https://gym.openai.com/docs/) and [source code](https://github.com/openai/gym).\n",
        "\n",
        "- **Python and NymPy**\n",
        "\n",
        "    The reader should have some knowledge of Python, NumPy array manipulation, and linear algebra. \n",
        "\n",
        "    To refresh the memory, you can take the [Python](https://docs.python.org/dev/tutorial/index.html) and [Linear algebra on n-dimensional arrays](https://numpy.org/doc/stable/user/tutorial-svd.html) tutorials. \n",
        "\n",
        "- **Deep learning and deep RL**\n",
        "\n",
        "    You should be familiar with main concepts of [deep learning](https://en.wikipedia.org/wiki/Deep_learning), which are explained in the [Deep learning](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) paper published in 2015 by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, who are regarded as some of the pioneers of the field. \n",
        "\n",
        "    The tutorial will try to guide you through the main concepts of deep RL and you will find various literature with links to original sources for your convenience.\n",
        "\n",
        "- **Jupyter notebook environments**\n",
        "\n",
        "    Because RL experiments can require high computing power, you can run the tutorial on the cloud for free using [Binder](https://mybinder.org) or [Google Colaboratory](https://colab.research.google.com/notebooks/intro.ipynb) (which offers free limited GPU and TPU acceleration).\n",
        "\n",
        "    This tutorial can also be run locally in an isolated environment, such as [Virtualenv](https://virtualenv.pypa.io/en/stable/). You can use [Jupyter Notebook or JupyterLab](https://jupyter.org/install) to run each notebook cell. Don't forget to [set up NumPy](https://numpy.org/doc/stable/user/absolute_beginners.html#installing-numpy) and [Gym](https://github.com/openai/gym#installation).\n",
        "\n",
        "    There are other dependencies for displaying data that are mainly for rendering a video game playback within a Jupyter notebook and this tutorial will show how to install and set them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z61twYBW4BMN"
      },
      "source": [
        "### Table of contents\n",
        "\n",
        "- A note on RL and deep RL\n",
        "- Deep RL glossary\n",
        "- About policy gradients\n",
        "1. Set up Pong\n",
        "- (Optional) Enable video playback in a notebook\n",
        "2. Preprocess frames (the observation)\n",
        "3. Create the policy (the neural network)\n",
        "4. Define the discounted rewards function\n",
        "5. Train the agent\n",
        "6. Next steps\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-b7IQTppMdF"
      },
      "source": [
        "### A note on RL and deep RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIqcGxNwyoCo"
      },
      "source": [
        "In [_RL_](https://en.wikipedia.org/wiki/Reinforcement_learning), your agent learns from trial and error by interacting with an environment using a so-called policy to gain experience. After taking one action, the agent receives information about its reward (which it may or may not get) and the next observation of the environment. It can then proceed to take another action. This happens over a number of episodes and/or until the task is deemed to be complete. The agent's policy works by \"mapping\" the agent's observations to its actions — that is, assigning a presentation of what the agent observes with required actions. The overall goal is usually to optimize the agent's policy such that it maximizes the expected rewards from each observation. \n",
        "\n",
        "In [supervised](https://en.wikipedia.org/wiki/Supervised_learning) deep learning for tasks, such as image recognition, language translation, or text classification, you're more likely to use a lot of labeled data. However, in RL, agents typically don't receive direct explicit feedback indicating correct or wrong actions — they rely on other signals, such as rewards. \n",
        "\n",
        "For detailed information about RL, there is an [introductory book](https://web.archive.org/web/20050806080008/http://www.cs.ualberta.ca/~sutton/book/the-book.html) by Richard Sutton and Andrew Barton.\n",
        "\n",
        "_Deep RL_ combines RL with [deep learning](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf). The field had its first major success in more complex environments, such as video games, in 2013 — a year after the [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) breakthrough in computer vision. Volodymyr Mnih and colleagues at DeepMind published a research paper called [Playing Atari with deep reinforcement learning](https://arxiv.org/abs/1312.5602) (and [updated](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) in 2015) that showed that they were able to train an agent that could play several classic games from the Arcade Learning Environment at a human-level. Their RL algorithm — called a deep Q-network (DQN) — used [convolutional layers](https://en.wikipedia.org/wiki/Convolutional_neural_network) in a neural network that approximated [Q learning](https://en.wikipedia.org/wiki/Q-learning) and used [experience replay](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). \n",
        "\n",
        "Since 2013, researchers have come up with many notable approaches for learning to solve complex tasks using deep RL, such as [AlphaGo](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ) for the game of Go (David Silver et al, 2016), [AlphaZero](http://science.sciencemag.org/cgi/content/full/362/6419/1140?ijkey=XGd77kI6W4rSc&keytype=ref&siteid=sci) that mastered Go, Chess, and Shogi with self-play (David Silver et al, 2017-2018), [OpenAI Five](https://arxiv.org/pdf/1912.06680.pdf) for Dota 2 with [self-play](https://openai.com/blog/competitive-self-play/) (OpenAI, 2019), and [AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/) for StarCraft 2 that used an [actor-critic](https://arxiv.org/pdf/1802.01561.pdf) algorithm with [experience replay](https://link.springer.com/content/pdf/10.1023%2FA%3A1022628806385.pdf), [self-imitation learning](http://proceedings.mlr.press/v80/oh18b/oh18b.pdf), and [policy distillation](https://arxiv.org/pdf/1511.06295.pdf) (Oriol Vinyals et al, 2019). In addition, there have been other experiments, such as deep RL for [Battlefield 1](https://www.ea.com/seed/news/self-learning-agents-play-bf1) by engineers at Electronic Arts/DICE.\n",
        "\n",
        "One of the reasons why video games are popular in deep RL research is that, unlike real-world experiments, such as RL with [remote-controlled helicopters](http://heli.stanford.edu/papers/nips06-aerobatichelicopter.pdf) ([Pieter Abbeel](https://www2.eecs.berkeley.edu/Faculty/Homepages/abbeel.html)  et al, 2006), virtual simulations can offer safer testing environments. \n",
        "\n",
        "But if you're interested in learning about the implications of deep RL on other fields, such as neuroscience, you can refer to a [paper](https://arxiv.org/pdf/2007.03750.pdf) by [Matthew Botvinick](https://www.youtube.com/watch?v=b0LddBiF5jM) et al (2020)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYKhl_w9i-vu"
      },
      "source": [
        "### Deep RL glossary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsmpxftYohEP"
      },
      "source": [
        "Below is a concise glossary of deep RL terms you may find useful for the remaining part of the tutorial:\n",
        "\n",
        "- In a finite-horizon world, such as a game of Pong, the learning agent can expore (and exploit) the _environment_ over an _episode_. It usually takes many episodes for the agent to learn.\n",
        "- The agent interacts with the _environment_ using _actions_.\n",
        "- After taking an action, the agent receives some feedback through a _reward_ (if there is any), depending on which action it takes and the _state_ it is in. The state contains information about vthe environment.\n",
        "- The agent's _observation_ is a partial observation of the state — this is the term this tutorial prefers (instead of state).\n",
        "- The agent can choose an action based on cumulative _rewards_ (also known as the _value function_), and the _policy_.\n",
        "- The _policy_ (defined by a neural network) outputs action choices (as (log) probabilities) that should maximize the cumulative rewards from the state the agent is in.\n",
        "- The _cumulative reward function_ estimates the quality of the observations the agent visits using its policy.\n",
        "- The _expected return from an observation_, conditional to the action, is called the _action-value_ function. To provide more weight to shorter-term rewards versus the longer-term ones, you usually use a _discount factor_ (often a floating point number between 0.9 and 0.99).\n",
        "- The sequence of actions and states (observations) during each policy \"run\" by the agent is sometimes referred to as a _trajectory_ — such a sequence yields _rewards_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JStsPFEoZU5"
      },
      "source": [
        "### About policy gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoKkh4i2ysB3"
      },
      "source": [
        "You will train your Pong agent through an \"on-policy\" method using policy gradients — it's an algorithm belonging to a family of _policy-based_ methods. Policy gradient methods typically update the parameters of the policy with respect to the long-term cumulative reward using [_gradient descent_](https://en.wikipedia.org/wiki/Gradient_descent) that is widely used in machine learning. \n",
        "\n",
        "And, since the goal is to maximize the function (the rewards), not minimize it, the process is also called _gradient ascent_.\n",
        "\n",
        "In comparison, DQN uses a type of \"off-policy\" _value-based_ method (that approximates Q learning), while the original [AlphaGo](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ) uses policy gradients and [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search).\n",
        "\n",
        "Policy gradients _with function approximation_, such as neural networks, were [written about](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) in 2000 by Richard Sutton et al. They were influenced by a number of previous works, including statistical gradient-following algorithms, such as [REINFORCE](https://www.semanticscholar.org/paper/Simple-statistical-gradient-following-algorithms-Williams/4c915c1eecb217c123a36dc6d3ce52d12c742614) (Ronald Williams, 1992), as well as [backpropagation](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf) (Geoffrey Hinton, 1986), which helps deep learning algorithms learn. RL with neural-network function approximation were introduced in the 1990s in research by Gerald Tesauro ([Temporal difference learning and td-gammon](https://dl.acm.org/doi/10.1145/203330.203343), 1995), who worked with IBM on an agent that learned to [play backgammon](https://en.wikipedia.org/wiki/TD-Gammon) in 1992, and Long-Ji Lin ([Reinforcement learning for robots using neural networks](https://dl.acm.org/doi/book/10.5555/168871), 1993)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sWBMi61EOok"
      },
      "source": [
        "### 1. Set up Pong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBNts22qfbbp"
      },
      "source": [
        "1. If you haven't already installed Gym, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzjJ7CVsfYlD"
      },
      "outputs": [],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7UEY24ACQVp"
      },
      "source": [
        "2. Import NumPy, OpenAI Gym and the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cWZsoNtKbFy"
      },
      "outputs": [],
      "source": [
        "# Import NumPy and OpenAI Gym.\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Gym can monitor and save the output using the Monitor wrapper.\n",
        "from gym import wrappers\n",
        "from gym.wrappers import Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk69Jer36elU"
      },
      "source": [
        "3. Instantiate a Gym environment for the game of Pong:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_2E9b6j5bkp"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Pong-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0M7me6d5mrK"
      },
      "source": [
        "4. Let's review which actions are available in the `Pong-v0` environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zEAzKEu5cU0"
      },
      "outputs": [],
      "source": [
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_kO52Hr5_5b"
      },
      "outputs": [],
      "source": [
        "print(env.get_action_meanings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbBxzJ0l6HDz"
      },
      "source": [
        "5. There are 6 actions. However, `LEFTFIRE` is actually `LEFT`, `RIGHTFIRE` — `RIGHT`, and `NOOP` — `FIRE`.\n",
        "\n",
        "> **Note**: For simplicity, you will have one output of your policy network —a (log) probability for action 2 (`RIGHT` or \"move up\"). The remaining action will be indexed at 3 (`LEFT` or \"mode down\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BvZr2TZCjAH"
      },
      "source": [
        "6. Gym can save videos of the agent's learning in an MP4 format — wrap `Monitor()` around the environment by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a34ZoOaTEttn"
      },
      "outputs": [],
      "source": [
        "env = Monitor(env, './video', force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q12PSiELDtF5"
      },
      "source": [
        "### (Optional) Enable video playback in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaJdSEm9DH1q"
      },
      "source": [
        "While you can perform all kinds of RL experiments in a Jupyter notebook, rendering images or videos of a Gym environment to visualize how your agent plays the game of Pong after training can be rather challenging.\n",
        "\n",
        "- If you're using [**Binder**](https://mybinder.org) — a free Jupyter notebook-based tool — you can set up the Docker image and add `freeglut3-dev`, `xvfb`, and `x11-utils` to the `apt.txt` configuration file to install the initial dependencies. Then, to `binder/environment.yml` under `channels`, add `gym`, `pyvirtualdisplay` and anything else you may need, such as `python=3.7`, `pip`, and `jupyterlab`. Check the following [post](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) for more information.\n",
        "\n",
        "- If you're using [**Google Colaboratory**](https://colab.research.google.com/notebooks/intro.ipynb) (another free Jupyter notebook-based tool), you can enable video playback of the game environments installing and setting up [X virtual frame buffer](https://en.wikipedia.org/wiki/Xvfb)/[Xvfb](https://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml), [X11](https://en.wikipedia.org/wiki/X_Window_System), [FFmpeg](https://ffmpeg.org), [PyVirtualDisplay](https://github.com/ponty/PyVirtualDisplay), [PyOpenGL](http://pyopengl.sourceforge.net), and other dependencies, as described further below.\n",
        "\n",
        "- If you're following the instructions in this tutorial in a local environment on Linux or macOS, you can add most of the code into one **Python (`.py`)** file. Then, you can run your Gym experiment through `python your-code.py` in your terminal. To enable rendering, you can use the command-line interface by following the [official OpenAI Gym documentation](https://github.com/openai/gym#rendering-on-a-server) (make sure you have Gym and Xvfb installed, as described in the guide)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlmEdwcBIxwE"
      },
      "source": [
        "If you're using Google Colaboratory, run the cells below to enable video playback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESBjrn4BDJFC"
      },
      "outputs": [],
      "source": [
        "# Install Xvfb and X11 dependencies.\n",
        "!apt-get install -y xvfb x11-utils > /dev/null 2>&1\n",
        "# To work with videos, install FFmpeg.\n",
        "!apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "# Install PyVirtualDisplay for visual feedback and other libraries/dependencies.\n",
        "!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3vGBj5jGwtg"
      },
      "outputs": [],
      "source": [
        "# The virtual display module.\n",
        "from pyvirtualdisplay import Display\n",
        "# For image rendering in the notebook.\n",
        "from IPython import display as ipythondisplay\n",
        "# To help with video rendering.\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Initialize the virtual buffer at 400x300 (adjustable size).\n",
        "# With Xvfb, you should set `visible=False`.\n",
        "display = Display(visible=False, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "# Check that no display is present.\n",
        "# If no displays are present, the expected output is `:0`.\n",
        "!echo $DISPLAY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KQRAgxmCVHU"
      },
      "outputs": [],
      "source": [
        "# A helper function to display videos in Jupyter notebooks:.\n",
        "# Source: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "def show_any_video(mp4video=0):\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[mp4video]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                                            loop controls style=\"height: 400px;\">\n",
        "                                            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                                            </video>'''.format(encoded.decode('ascii'))))\n",
        "        \n",
        "    else:\n",
        "        print('Could not find the video!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suquqXhuFzvm"
      },
      "source": [
        "### 2. Preprocess frames (the observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKCxQtwlOtlK"
      },
      "source": [
        "In this section you will set up a function to preprocess the input data (game observation) to make it digestible for the neural network, which can only work with inputs that are in a form of tensors (multidimensional arrays) of floating-point type.\n",
        "\n",
        "Similar to DeepMind's DQN method, your agent will use the frames from the Pong game — pixels from screen frames — as input-observations for the policy network. The game observation tells the agent about where the ball is before it is fed (with a forward pass) into the neural network (the policy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQk0POHw0BJ9"
      },
      "source": [
        "1. Check that the Pong's observations are of Gym `Box` instance and encoded with 210x160 pixels over 3 (red, green and blue) dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqKk7lvKO5QJ"
      },
      "outputs": [],
      "source": [
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieay1E9_0Cwg"
      },
      "source": [
        "> In Gym, the agent's actions and observations can be part of the `Box` (n-dimensional) or `Discrete` (fixed-range integers) classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4bk0Ntg0uQc"
      },
      "source": [
        "Pong screen frames are 210x180x3 with data type of `uint8` (or 8-bit integers) and you need to convert them into 1D grayscale vectors with 6,400 (80x80x1) floating point arrays. \n",
        "\n",
        "To flatten into 1D arrays, you will use NumPy's [`np.ravel()`](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viGrCqkRaDdk"
      },
      "source": [
        "2. Let's set up a helper function for frame (observation) preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLAndceaFvGl"
      },
      "outputs": [],
      "source": [
        "def frame_preprocessing(observation_frame):\n",
        "  # Crop the frame.\n",
        "  observation_frame = observation_frame[35:195]\n",
        "  # Downsample the frame by a factor of 2.\n",
        "  observation_frame = observation_frame[::2,::2,0]\n",
        "  # Remove the background and apply other enhancements.\n",
        "  observation_frame[observation_frame == 144] = 0\n",
        "  observation_frame[observation_frame == 109] = 0\n",
        "  observation_frame[observation_frame != 0] = 1\n",
        "  # Return the preprocessed frame as a 1D floating-point array.\n",
        "  return observation_frame.astype(np.float).ravel()"
      ]
    },
    {
      "source": [
        "### 3. Create the policy (the neural network)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO7Gzk-ScV45"
      },
      "source": [
        "Next, you will define the policy as a simple feedforward network that uses a game observation as an input and outputs an action log probability:\n",
        "\n",
        "- For the _input_, it will use the Pong video game frames — the preprocessed 1D vectors with 6,400 (80x80) floating point arrays.\n",
        "- The _hidden layer_ will compute the weighted sum of inputs using NumPy's dot product function [`np.dot()`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) for the arrays and then apply a _nonlinear activation function_, such as [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). \n",
        "- Then, the _output layer_ will perform the matrix-multiplication again of  weight parameters and the hidden layer's output (with [`np.dot()`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)), and send that information through a [softmax](https://en.wikipedia.org/wiki/Softmax_function) _activation function_.\n",
        "- In the end, the policy network will output one action log probability (given that observation) for the agent — the probability for Pong action indexed in the environment at 2 (\"moving the racket up\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTVDSuf5qY2E"
      },
      "source": [
        "1. Instantiate certain parameters for the input, hidden, and output layers, and start setting up the network model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A78ihWDHcQvJ"
      },
      "outputs": [],
      "source": [
        "# Set the input (observation) dimensionality - your preprocessed screen frames.\n",
        "D = 80 * 80\n",
        "# Set the number of hidden layer neurons.\n",
        "H = 200\n",
        "# Instantiate your neural network model as an empty dictionary.\n",
        "model = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCG3g4s-Fapk"
      },
      "source": [
        "In a neural network, _weights_ are important adjustable parameters that the network fine-tunes by forward and backward propagating the data.\n",
        "\n",
        "2. Using a technique called [Xavier initialization](https://www.deeplearning.ai/ai-notes/initialization/#IV), set up the network model's initial weights with NumPy's [`np.random.randn()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html) that return random numbers over a standard Normal distribution, as well as [`np.sqrt()`](https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html?highlight=numpy.sqrt#numpy.sqrt):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh2pUHZ6FtUe"
      },
      "outputs": [],
      "source": [
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D)\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4J5Elsiq5Qk"
      },
      "source": [
        "Your policy network starts by randomly initializing the weights and feeds the input data (frames) forward from the input layer through a hidden layer to the output layers. This process is called the _forward pass_ or _forward propagation_ (`policy_forward()`\n",
        "\n",
        "3. Define the policy's forward pass/propagation function that receives preprocessed game observation (frames) as input: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV4bIYAsKd9o"
      },
      "outputs": [],
      "source": [
        "def policy_forward(x):\n",
        "    # Matrix-multiply the weights by the input in the one and only hidden layer.\n",
        "    h = np.dot(model['W1'], x)\n",
        "    # Apply non-linearity with ReLU.\n",
        "    h[h<0] = 0\n",
        "    # Calculate the \"dot\" product in the outer layer.\n",
        "    # The input for the sigmoid function is called logit.\n",
        "    logit = np.dot(model['W2'], h)\n",
        "    # Apply the sigmoid function for non-linearity.\n",
        "    p = sigmoid(logit)\n",
        "    # Return a log probability for the action 2 (\"move up\") \n",
        "    # and the hidden \"state\" that you need for backpropagation.\n",
        "    return p, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D14eeV27rMjb"
      },
      "source": [
        "> **Note**: There are two _activation functions_ for determining non-linear relationships between inputs and outputs and these [non-linear functions](https://en.wikipedia.org/wiki/Activation_function) are applied to the output of the hidden and output layers.\n",
        "> - You are applying a [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) — defined using one line in the code above — to the hidden layer's output, as well as [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) for the output layer that returns an action log probability.\n",
        "> \n",
        "\n",
        "4. Define the sigmoid function separately with NumPy's [`np.exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html?highlight=numpy.exp#numpy.exp) for computing exponentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pF2-D8kr6H5"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jr6WODRs3b0"
      },
      "source": [
        "During learning in deep RL, you use the action log probabilities (given an observation) and the discounted returns (for example, +1 or -1 in Pong) and perform the backwards pass to update the parameters — the policy network's weights.\n",
        "\n",
        "5. Let's define the _backward pass_ or _backpropagation_ function (`policy_backward()`) with the help of NumPy's modules for array multiplication — [`np.dot()`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html?highlight=numpy.dot#numpy.dot), outer product computation — [`np.outer()`](https://numpy.org/doc/stable/reference/generated/numpy.outer.html), and [`np.ravel()`](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html) — to flatten arrays into 1D arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPzzDkWXcPAX"
      },
      "outputs": [],
      "source": [
        "def policy_backward(eph, epdlogp):\n",
        "    # Using the intermediate hidden \"states\" of the network (`eph`)\n",
        "    # and the gradients of action log probabilities (`epdlogp`) for an episode,\n",
        "    # propagate the gradients back through the policy network\n",
        "    # and update the weights.\n",
        "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "    dh = np.outer(epdlogp, model['W2'])\n",
        "    dh[eph <= 0] = 0\n",
        "    dW1 = np.dot(dh.T, epx)\n",
        "    # Return new \"optimized\" weights for the policy network.\n",
        "    return {'W1':dW1, 'W2':dW2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIfnX6oj_WJT"
      },
      "source": [
        "6. When applying backpropagation during agent training, you will need to save several variables for each episode. Let's instantiate empty lists to store them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzOOhruG-UyJ"
      },
      "outputs": [],
      "source": [
        "# All preprocessed observations for the episode.\n",
        "xs = []\n",
        "# All hidden \"states\" (from the network) for the episode.\n",
        "hs = []\n",
        "# All gradients of probability actions \n",
        "# (with respect to observations) for the episode.\n",
        "dlogps = []\n",
        "# All rewards for the episode.\n",
        "drs = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNYJi11hVIAd"
      },
      "source": [
        "> **Note**: You will reset these variables manually at the end of each episode during training after they are \"full\" and reshaped with NumPy's [`np.vstack()`](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html). This is demonstrated in the training stage towards the end of the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-JgcydYZjuZ"
      },
      "source": [
        "7. To perform a gradient ascent when optimizing the agent's policy, it is common to use deep learning _optimizers_. In this example, you'll use [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp). It is an adaptive optimization method and, before the training begins, you should set a discounting factor for it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVtNth_1hNX0"
      },
      "outputs": [],
      "source": [
        "decay_rate = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAaR0PmE_0I0"
      },
      "source": [
        "8. You will also need to store the gradients (with the help of NumPy's [`np.zeros_like()`](https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html)) for the optimization step during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL8aMfCgWP-l"
      },
      "outputs": [],
      "source": [
        "# Save the update buffers that add up gradients over a batch.\n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() }\n",
        "# Store RMSProp memory for the RMSProp optimizer for gradient ascent.\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpOlBlNYEc3F"
      },
      "source": [
        "### 4. Define the discounted rewards function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4S7KcTWdyLE"
      },
      "source": [
        "1. Here, you need to set up a function for computing discounted rewards (`discount_rewards()`) that use a 1D array of rewards as inputs (with the help of NumPy's [`np.zeros_like()`](https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot1Gn4qsGEXs"
      },
      "outputs": [],
      "source": [
        "def discount_rewards(r):\n",
        "    # Initialize the discounted rewards as an array of zeros.\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    # From the last reward to the first...\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        # ...reset the reward sum\n",
        "        if r[t] != 0: running_add = 0\n",
        "        # ...compute the discounted reward (where `gamma` is the discount rate).\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    # Return the total discounted reward.\n",
        "    return discounted_r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hEkk0BXJBEb"
      },
      "source": [
        "> **Note**: In Pong, if a player doesn't hit the ball back, they receive a negative reward (-1) and the other one gets a +1 reward.\n",
        ">\n",
        "> The rewards that the agent receives by playing Pong have a significant variance. Therefore, it's best practice to normalize them with the same mean (using [`np.mean()`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) and standard deviation (using NumPy's [`np.std()`](https://numpy.org/doc/stable/reference/generated/numpy.std.html?highlight=std)). You will see this step in the training loop at the end of the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTZgxnNYenIC"
      },
      "source": [
        "2. As mentioned before, to provide more weight to shorter-term rewards, you will use a discount factor (gamma) that is often a number between 0.9 and 0.99:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmjK5dunetx9"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5t9Fls-GhB-"
      },
      "source": [
        "### 5. Train the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7HbH5M8Cm9"
      },
      "source": [
        "This section covers how to set up the training process during which your agent will be learning to play Pong using its policy.\n",
        "\n",
        "The pseudocode for the policy gradient method for Pong:\n",
        "\n",
        "- Instantiate the policy — your neural network — and randomly initialize the weights in the policy network.\n",
        "- Initialize a random observation.\n",
        "- Randomly initialize the weights in the policy network.\n",
        "- Repeat indefinitely:\n",
        "    - Input an observation into the policy network and output action probabilities for the agent (forward propagation).\n",
        "    - The agent takes an action for each observation, observes the received rewards and collects trajectories (every 10 episodes) of state-action experiences.\n",
        "    - For every batch of episodes:\n",
        "        - Compute the cumulative return and discount it to present.\n",
        "        - Perform gradient ascent (backpropagation) to optimize the policy network's parameters (its weights) and maximize the rewards.\n",
        "            - Maximize the probability of actions that led to high rewards.\n",
        "\n",
        "Since the example is not limited by the number of episodes, you can stop the training at any time or/and check saved MP4 videos of saved plays on your disk in the `/video` directory.\n",
        "\n",
        "> **Note**: \n",
        ">\n",
        "> When using only NumPy, the deep RL training process, including backpropagation, spans several lines of code that may appear quite long. One of the main reasons for this is you're not using a deep learning framework with an automatic differentiation library that usually simplifies such experiments.\n",
        ">\n",
        "> This example shows how to perform everything from scratch but you can also use one of many Python-based frameworks with \"autodiff\" and \"autograd\", which you will learn about at the end of the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD6XBqUqfNOV"
      },
      "source": [
        "1. Set the remaining parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKLLYUKbG-5A"
      },
      "outputs": [],
      "source": [
        "# How often (in episodes) to perform a parameter update.\n",
        "batch_size = 10\n",
        "# And the learning rate.\n",
        "learning_rate = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORj7JFGB0Gy8"
      },
      "source": [
        "> **Note**: \n",
        ">\n",
        "> - The _batch size_ is the number of times — in episodes — your agent can collect the state-action trajectories. At the end of the collection, you can perform the maximization of action-probability multiples.\n",
        "> \n",
        "> - The [_learning rate_](https://en.wikipedia.org/wiki/Learning_rate) helps limit the magnitude of weight updates to prevent them from overcorrecting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V3qqwmsmRtT"
      },
      "source": [
        "2. Start the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-WL_FAE1hI0"
      },
      "outputs": [],
      "source": [
        "# Set the game rendering default variable.\n",
        "render = False\n",
        "\n",
        "# Set the agent's initial (random) observation by calling `reset()`.\n",
        "observation = env.reset()\n",
        "\n",
        "# Initialize the previous observation\n",
        "prev_x = None \n",
        "\n",
        "running_reward = None\n",
        "reward_sum = 0\n",
        "episode_number = 0\n",
        "\n",
        "# Begin training the agent by initializing the training loop.\n",
        "while True:\n",
        "    # (For rendering.)\n",
        "    if render: env.render()\n",
        "\n",
        "    # 1. Forward propagation/forward pass:\n",
        "    # - Preprocess the observation (a game frame).\n",
        "    cur_x = frame_preprocessing(observation)\n",
        "    # - To simulate motion between the frames, \n",
        "    # set the input for the policy network\n",
        "    # as the difference between the current and previous preprocessed frames\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    # and set the previous observation (frame) as the current frame.\n",
        "    prev_x = cur_x\n",
        "    # - Perform forward pass in the policy network using preprocessed input, \n",
        "    # and save the action log probability and a hidden \"state\" (for backpropagation).\n",
        "    aprob, h = policy_forward(x)\n",
        "    # - Let the action 2 (\"move up\") be that probability\n",
        "    # if it's higher than a randomly sampled value\n",
        "    # or use action 3 (\"move down\") otherwise.\n",
        "    action = 2 if np.random.uniform() < aprob else 3\n",
        "\n",
        "    # 2. Save the observations and hidden \"states\" (from the network)\n",
        "    # for backpropagation.\n",
        "    xs.append(x)\n",
        "    hs.append(h)\n",
        "    \n",
        "    # 3. Differentiate the loss function for the backward pass:\n",
        "    # - If the action was to \"move up\" (index 2):\n",
        "    y = 1 if action == 2 else 0\n",
        "    # (Recall you used the sigmoid function to output an `aprob` action probability.)\n",
        "    # - Append the gradients of your action log probability.\n",
        "    dlogps.append(y - aprob)\n",
        "    # 4. Update the parameters with Gym's `step()` and obtain a new observation.\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # 5. Update the total sum of rewards.\n",
        "    reward_sum += reward\n",
        "    # 6. Append the reward for the previous action.\n",
        "    drs.append(reward)\n",
        "\n",
        "    # After an episode is finished:\n",
        "    if done:\n",
        "        # 1. Increment the episode count.\n",
        "        episode_number += 1\n",
        "        # 2. Reshape stored values with `np.vstack()` of:\n",
        "        # - Observation frames (inputs),\n",
        "        epx = np.vstack(xs)\n",
        "        # - hidden \"states\" (from the network),\n",
        "        eph = np.vstack(hs)\n",
        "        # - gradients of action log probability,\n",
        "        epdlogp = np.vstack(dlogps)\n",
        "        # - and rewards for the past episode.\n",
        "        epr = np.vstack(drs)\n",
        "\n",
        "        # 3. Reset the stored variables for the new episode:\n",
        "        # - Preprocessed observations.\n",
        "        xs = []\n",
        "        # - Hidden \"states\" (from the network).\n",
        "        hs = []\n",
        "        # - Gradients of the action log probabilities (with respect to observations).\n",
        "        dlogps = []\n",
        "        # - Rewards for the episode.\n",
        "        drs = []\n",
        "\n",
        "        # 4. Discount the rewards for the past episode using the helper \n",
        "        # function you defined earlier...\n",
        "        discounted_epr = discount_rewards(epr)\n",
        "        # ...and normalize the rewards, since the rewards have high variance.\n",
        "        discounted_epr -= np.mean(discounted_epr)\n",
        "        discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "        # 5. Multiply the discounted rewards by the gradients of the action log probabilities.\n",
        "        epdlogp *= discounted_epr\n",
        "        # 6. Perform backpropagation and gradient ascent.\n",
        "        # - Using given the hidden \"states\" of the network\n",
        "        # and the gradients of the action log probabilities.\n",
        "        # and update the weights.\n",
        "        grad = policy_backward(eph, epdlogp)\n",
        "        # - Save the policy gradients in a buffer.\n",
        "        for k in model: grad_buffer[k] += grad[k]\n",
        "        # - Use the RMSProp optimizer to perform the policy network\n",
        "        # parameter (weight) update at every batch size \n",
        "        # (by default: every 10 episodes).\n",
        "        if episode_number % batch_size == 0:\n",
        "            for k,v in model.items():\n",
        "                # The gradient.\n",
        "                g = grad_buffer[k]\n",
        "                # Use the RMSProp discounting factor.\n",
        "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "                # Update the policy network with a learning rate.\n",
        "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "                # Reset the gradient buffer at the end.\n",
        "                grad_buffer[k] = np.zeros_like(v)\n",
        "\n",
        "        # To display in the output during training:\n",
        "        # Measure the total discounted reward.\n",
        "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "        print ('Resetting the Pong environment. Episode total reward: {} Running mean: {}'.format(reward_sum, running_reward))\n",
        "\n",
        "        # Set the reward sum back to 0 for the next episode.\n",
        "        reward_sum = 0\n",
        "\n",
        "        # Set the agent's initial observation by calling `reset()` \n",
        "        # for the next episode.\n",
        "        observation = env.reset()\n",
        "        prev_x = None\n",
        "\n",
        "    # To display the output during training.\n",
        "    # If the reward is +1 or -1 when the game ends:\n",
        "    if reward != 0:\n",
        "        print ('Episode {}: Game finished. Reward: {}...'.format(episode_number, reward) + ('' if reward == -1 else ' POSITIVE REWARD!'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tv--3o01jsC"
      },
      "source": [
        "If you have previously run an experiment and want to run it again, you `Monitor` instance may still be running — uncomment and run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Yt1HbAysJq"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azfs1PjVlyqf"
      },
      "source": [
        "(Optional) If you want to view the last (very quick) gameplay inside a Jupyter notebook and implemented the `show_any_video()` function earlier, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReCNElr8CC90"
      },
      "outputs": [],
      "source": [
        "show_any_video(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3v_JkYGoAlv"
      },
      "source": [
        "In comparison, Andrej Karpathy's experiment took about 8,000 episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwyhet51Dnmd"
      },
      "source": [
        "### Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDZnzEOk1kNA"
      },
      "source": [
        "You may notice that training an RL agent takes a long time, depending on the hardware — CPUs and GPUs — you are using for this task. \n",
        "\n",
        "Policy gradient methods can learn a task if you give them a lot of time and optimization in RL is a challenging problem. Training agents to learn to play Pong or any other task can be sample-inefficient and require a lot of episodes. You may also notice in your training output that even after hundreds of episodes, the rewards may have high variance.\n",
        "\n",
        "In addition, like in many deep learning-based algorithms, you should take into account a large amount of parameters that your policy has to learn. In Pong, this number adds up to 1 million or more with 200 nodes in the hidden layer of the network and the input dimension being of size 6,400 (80x80). Therefore, adding more CPUs and GPUs to assist with training can always be an option.\n",
        "\n",
        "You can use a much more advanced policy gradient-based algorithm that can help speed up training, improve the sensitivity to parameters, and resolve other issues. For example, there are \"self-play\" methods, such as [Proximal Policy Optimization (PPO)](https://arxiv.org/pdf/1707.06347) developed by [John Schulman](http://joschu.net) et al in 2017, which were [used](https://openai.com/blog/openai-five/#rapid) to train the [OpenAI Five](https://arxiv.org/pdf/1912.06680.pdf) agent over 10 months to play Dota 2 at a competitive level. Of course, if you apply these methods to smaller Gym environments, it should take hours, not months to train.\n",
        "\n",
        "In general, there are many RL challenges and possible solutions and you can explore some of them in [Reinforcement learning, fast and slow](https://static1.squarespace.com/static/555aab07e4b03e184ddaf731/t/5f5245cb100273193b14548a/1599227416572/TICS__RL_Fast_and_Slow_accepted.pdf) by [Matthew Botvinick](https://hai.stanford.edu/people/matthew-botvinick), Sam Ritter, [Jane X. Wang](http://www.janexwang.com), Zeb Kurth-Nelson, [Charles Blundell](http://www.gatsby.ucl.ac.uk/~ucgtcbl/), and [Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis) (2019).\n",
        "\n",
        "---\n",
        "\n",
        "If you want to learn more about deep RL, you can check free educational material on [Spinning Up in Deep RL](https://openai.com/blog/spinning-up-in-deep-rl/) developed by OpenAI and lectures taught by practitioners at [DeepMind](https://www.youtube.com/c/DeepMind/videos) and [UC Berkeley](https://www.youtube.com/channel/UC4e_-TvgALrwE1dUPvF_UTQ/videos).\n",
        "\n",
        "Finally, you can go beyond NumPy with specialized frameworks and APIs — such as [TensorFlow](https://www.tensorflow.org/guide/tf_numpy?hl=el), [PyTorch](https://pytorch.org/docs/stable/generated/torch.from_numpy.html), Swift for TensorFlow (with [Python interoperability](https://www.tensorflow.org/swift/tutorials/python_interoperability)), and [JAX](https://github.com/google/jax) — that support NumPy, have built-in [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), and are designed for high-performance numerical computing and machine learning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tutorial-deep-reinforcement-learning-with-pong-from-pixels.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
